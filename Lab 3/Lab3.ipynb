{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "\n",
    "Authors: Riley Galante, Jeffrey Taylor, Eric Bernard, Austin Hayden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Task and Business Application\n",
    "\n",
    "### Business Understanding\n",
    "\n",
    "This data can be useful in determining the age and content rating of a video game with the goal of being able to classify a video game with the appropriate rating it would receive before entering the market.\n",
    "\n",
    "This dataset contains 1,895 unique video games that have already been rated by the Entertainment Software Rating Board (ESRB). The ESRB rates video games on the following scale: Rating Pending (RP), Early Childhood (EC), Everyone (E), Everyone 10+ (E 10+), Teen (T), Mature (M), and Adult (A). In this dataset, only games rated E, E 10+, T, and M are represented, so our classifier will be limited to these ratings. It also contains many important features that contribute to the certain rating a video game receives. These features are represented as binary values where 0 relates to the video game not having that characteristic, and 1 relates to the video game having that attribute. Some of the important features that will help classify the video game include Alcohol References, Blood and Gore, Crude Humor, Language, Nudity, Use of Drugs and Alcohol, Violence, etc.\n",
    "\n",
    "The ratings given to the video game determine age restrictions on who can purchase the game and also the intended audience of the video game.\n",
    "\n",
    "### Third-party Interest\n",
    "\n",
    "Because our end goal is to be able to classify a video game to an ESRB rating based on whether the game has certain features or not, game developers would be interested in this classification algorithm. The algorithm would be useful to them because they would be able to have insight into the rating the game will receive before it is given its official rating by the ESRB. Therefore, game developers can tweak their game according to the intended audience of the game. For example, if a game is being developed for everyone but receives a rating of Teen from our algorithm, then the game developers could look at removing or reducing certain features of the game to bring down the rating. Furthermore, this algorithm will be useful in marketing the video game. By having an insight into the rating the game will receive by the ESRB, game developers can go ahead and begin marketing the video game to the audience of the rating it is classified as, while it still has not been officially rated.\n",
    "\n",
    "The use of this algorithm would be offline since there is no real time advantage of having the model deployed. Game developers will use the classification algorithm when they have a game ready to be rated or whenever they want to test what a certain combination of features of a game will be rated as.\n",
    "\n",
    "### Measure of Success\n",
    "\n",
    "Since the classification algorithm is not for the official rating and only used as an insight into the official rating, we think that a success rate of 80% or greater would be considered useful for game developers. Having a success rate of greater than 80% will give game developers confidence in our algorithm that the rating it is classified as will most likely be the official rating it receives.\n",
    "\n",
    "Any misclassifications will not result in major concern to the game developers. The only real negative to a misclassification would be a waste in resources marketing to the wrong audience. Misclassifications are expected since we are trying to predict the decision of the ESRB. As long as misclassifications are minimal, game developers will still have faith in our classification algorithm.\n",
    "\n",
    "Dataset source: https://www.kaggle.com/imohtn/video-games-rating-by-esrb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alcohol_reference           int64\n",
      "animated_blood              int64\n",
      "blood                       int64\n",
      "blood_and_gore              int64\n",
      "cartoon_violence            int64\n",
      "crude_humor                 int64\n",
      "drug_reference              int64\n",
      "fantasy_violence            int64\n",
      "intense_violence            int64\n",
      "language                    int64\n",
      "lyrics                      int64\n",
      "mature_humor                int64\n",
      "mild_blood                  int64\n",
      "mild_cartoon_violence       int64\n",
      "mild_fantasy_violence       int64\n",
      "mild_language               int64\n",
      "mild_lyrics                 int64\n",
      "mild_suggestive_themes      int64\n",
      "mild_violence               int64\n",
      "no_descriptors              int64\n",
      "nudity                      int64\n",
      "partial_nudity              int64\n",
      "sexual_content              int64\n",
      "sexual_themes               int64\n",
      "simulated_gambling          int64\n",
      "strong_janguage             int64\n",
      "strong_sexual_content       int64\n",
      "suggestive_themes           int64\n",
      "use_of_alcohol              int64\n",
      "use_of_drugs_and_alcohol    int64\n",
      "violence                    int64\n",
      "dtype: object\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1895 entries, 0 to 1894\n",
      "Data columns (total 31 columns):\n",
      " #   Column                    Non-Null Count  Dtype\n",
      "---  ------                    --------------  -----\n",
      " 0   alcohol_reference         1895 non-null   int64\n",
      " 1   animated_blood            1895 non-null   int64\n",
      " 2   blood                     1895 non-null   int64\n",
      " 3   blood_and_gore            1895 non-null   int64\n",
      " 4   cartoon_violence          1895 non-null   int64\n",
      " 5   crude_humor               1895 non-null   int64\n",
      " 6   drug_reference            1895 non-null   int64\n",
      " 7   fantasy_violence          1895 non-null   int64\n",
      " 8   intense_violence          1895 non-null   int64\n",
      " 9   language                  1895 non-null   int64\n",
      " 10  lyrics                    1895 non-null   int64\n",
      " 11  mature_humor              1895 non-null   int64\n",
      " 12  mild_blood                1895 non-null   int64\n",
      " 13  mild_cartoon_violence     1895 non-null   int64\n",
      " 14  mild_fantasy_violence     1895 non-null   int64\n",
      " 15  mild_language             1895 non-null   int64\n",
      " 16  mild_lyrics               1895 non-null   int64\n",
      " 17  mild_suggestive_themes    1895 non-null   int64\n",
      " 18  mild_violence             1895 non-null   int64\n",
      " 19  no_descriptors            1895 non-null   int64\n",
      " 20  nudity                    1895 non-null   int64\n",
      " 21  partial_nudity            1895 non-null   int64\n",
      " 22  sexual_content            1895 non-null   int64\n",
      " 23  sexual_themes             1895 non-null   int64\n",
      " 24  simulated_gambling        1895 non-null   int64\n",
      " 25  strong_janguage           1895 non-null   int64\n",
      " 26  strong_sexual_content     1895 non-null   int64\n",
      " 27  suggestive_themes         1895 non-null   int64\n",
      " 28  use_of_alcohol            1895 non-null   int64\n",
      " 29  use_of_drugs_and_alcohol  1895 non-null   int64\n",
      " 30  violence                  1895 non-null   int64\n",
      "dtypes: int64(31)\n",
      "memory usage: 459.1 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('Video_games_esrb_rating.csv')\n",
    "\n",
    "# map the esrb string values to integers for our target\n",
    "rating = []\n",
    "\n",
    "for i in df['esrb_rating']:\n",
    "    if(i == 'E'):\n",
    "        rating.append(0)\n",
    "    elif(i == 'ET'):\n",
    "        rating.append(1)\n",
    "    elif(i == 'T'):\n",
    "        rating.append(2)\n",
    "    else:\n",
    "        rating.append(3)\n",
    "        \n",
    "del df['title'] #get rid of unnecessary data\n",
    "del df['console']\n",
    "del df['esrb_rating'] #get rid of original column since its been mapped now\n",
    "\n",
    "print(df.dtypes)\n",
    "print(\"\\n{:+^60s}\\n\".format(\"\"))\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Dataset Description\n",
    "For this dataset, the first 31 columns are descriptors that go into the esrb rating for a video game. All those colums are binary data, where 1 represents a video game that has that particular attribute and 0 means it does not. Because the esrb rating was in string format, we one-hot encoded it so that the logistic regression would be able to successfully predict this categorical data. This is seen in the rating_E, rating_ET, rating_M, and rating_T columns that were added, representing E for everyone, E 10+, M for Mature, and T for Teen respectively. Additionally, we removed unnecessary information from the dataset that would not help our prediction, like the titles and console of the video games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1895,)\n",
      "[0 1 3 ... 0 2 0]\n",
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n",
      "1895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# E, ET, M, T = df['rating_E'].to_numpy(), df['rating_ET'].to_numpy(), df['rating_M'].to_numpy(), df['rating_T'].to_numpy()\n",
    "\n",
    "#stack rating encodings into one array for target array\n",
    "# y = np.vstack((E, ET, M, T)).T\n",
    "# yf = list(map(lambda i: str(i), y))\n",
    "# t_str = ''\n",
    "# for x in range(len(y)):\n",
    "#     t_str = yf[x]\n",
    "#     t = t_str.replace('[','')\n",
    "#     t = t_str.replace(']','')\n",
    "#     t = t_str.replace(' ','')\n",
    "#     print(int(t[0],2))\n",
    "\n",
    "y = np.array(rating).T\n",
    "\n",
    "#remaining columns in dataset are used for X\n",
    "X = df.to_numpy()\n",
    "\n",
    "print(y.shape)\n",
    "print(y)\n",
    "\n",
    "#set up shuffle split using example\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits = num_cv_iterations, test_size = 0.2)\n",
    "                         \n",
    "print(cv_object)\n",
    "print(num_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 80/20 train-test split is appropriate for our data set because it is not dependent on ordering or a certain time factor. Regardless of which are chosen for training or testing, all instances in our dataset use just the particular descriptors for that game to determine the rating, nothing more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Binary Logisitic Regression that Other Classes will Inherit\n",
    "Binary Log Reg, or versions which will inherit off it, will be used once problem is simplified to binary as part of one vs all structure in the main multi class classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic binary logistic regression with gradient descent from class examples\n",
    "from scipy.special import expit\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, norm, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.norm = norm\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        \n",
    "        # L1 Norm takes absolute value of all w, so multiply w/|w| to C\n",
    "        if(self.norm == '1'):\n",
    "            gradient[1:] -= (self.w_[1:]/(abs(self.w_[1:])+.001)) * self.C\n",
    "        # L2 Norm takes squared value of all w, so multiply 2w to C\n",
    "        elif(self.norm == '2'):\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        # L1 and L2 Norm\n",
    "        elif(self.norm == '3'):\n",
    "            gradient[1:] -= (self.w_[1:]/(abs(self.w_[1:])+.001)) * self.C\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C \n",
    "        #else no regularization\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "            # add bacause maximizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest Gradient Descent Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steepest descent from in class examples\n",
    "# and we can update this to use a line search along the gradient like this:\n",
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "from numpy import ma # (masked array) this has most numpy functions that work with NaN data.\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    def __init__(self, line_iters=5, **kwds):        \n",
    "        self.line_iters = line_iters\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(eta,X,y,w,grad,C,self):\n",
    "        wnew = w - grad*eta\n",
    "        g = expit(X @ wnew)\n",
    "        \n",
    "        if(self.norm == '1'):\n",
    "            return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(abs(wnew))\n",
    "        elif(self.norm == '2'):\n",
    "            return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(wnew**2)\n",
    "        elif(self.norm == '3'):\n",
    "            return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(abs(wnew)) + C*sum(wnew**2)\n",
    "        #else no regularization\n",
    "        else:\n",
    "            return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0]))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = -self._get_gradient(Xb,y)\n",
    "            # minimization inopposite direction\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.line_iters} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.objective_function, # objective function to optimize\n",
    "                                  bounds=(0,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient,self.C,self), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ -= gradient*eta # set new function values\n",
    "            # subtract to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD from class examples\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        \n",
    "        # L1 Norm takes absolute value of all w, so multiply w/|w| to C\n",
    "        if(self.norm == '1'):\n",
    "            gradient[1:] -= (self.w_[1:]/(abs(self.w_[1:])+.001)) * self.C\n",
    "        # L2 Norm takes squared value of all w, so multiply 2w to C\n",
    "        elif(self.norm == '2'):\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        # L1 and L2 Norm\n",
    "        elif(self.norm == '3'):\n",
    "            gradient[1:] -= (self.w_[1:]/(abs(self.w_[1:])+.001)) * self.C\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C \n",
    "        #else no regularization\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS Quasi-Newton Method Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFGS from in class example\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from numpy import ma\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_norm(w,C):\n",
    "        if(self.norm == '1'):\n",
    "            return C*sum(abs(w))\n",
    "        elif(self.norm == '2'):\n",
    "            return C*sum(w**2)\n",
    "        elif(self.norm == '3'):\n",
    "            return C*sum(abs(w))+C*sum(w**2)\n",
    "        #else no regularization\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C,self):\n",
    "        g = expit(X @ w)\n",
    "        \n",
    "        if(self.norm == '1'):\n",
    "            return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(abs(w))\n",
    "        elif(self.norm == '2'):\n",
    "            return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(w**2)\n",
    "        elif(self.norm == '3'):\n",
    "            return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(abs(w)) + C*sum(w**2)\n",
    "        #else no regularization\n",
    "        else:\n",
    "            return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0]))\n",
    "        \n",
    "        # invert this because scipy minimizes, but we derived all formulas for maximzing\n",
    "        #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C,self):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        \n",
    "        # L1 Norm takes absolute value of all w, so multiply w/|w| to C\n",
    "        if(self.norm == '1'):\n",
    "            gradient[1:] -= (w[1:]/(abs(w[1:])+.001)) * self.C\n",
    "        # L2 Norm takes squared value of all w, so multiply 2w to C\n",
    "        elif(self.norm == '2'):\n",
    "            gradient[1:] += -2 * w[1:] * self.C\n",
    "        # L1 and L2 Norm\n",
    "        elif(self.norm == '3'):\n",
    "            gradient[1:] -= (w[1:]/(abs(w[1:])+.001)) * self.C\n",
    "            gradient[1:] += -2 * w[1:] * self.C \n",
    "        #else no regularization\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C,self), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi class logistic regression template from class example\n",
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, norm, iterations=20, \n",
    "                 C=0.0001, \n",
    "                 solver=LineSearchLogisticRegression):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.norm = norm\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = np.array(y==yval).astype(int) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            \n",
    "            lr = self.solver(eta=self.eta,norm=self.norm,iterations=self.iters,C=self.C)\n",
    "            lr.fit(X,y_binary)\n",
    "\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(lr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for lr in self.classifiers_:\n",
    "            probs.append(lr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along rowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training And Testing Based On User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter which optimization technique should be used: 1 - Steepest Descent, 2 - Stochastic, 3 - Quasi-Newton(BFGS) 1\n",
      "Enter which regularization technique should be used: 1 - L1 Regularization, 2 - L2 Regularization, 3 - L1 and L2 Regularization, 4 - No Regularization 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1468dd5dc7544c568a5172dee286ec9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='cost', options=(0.0001, 0.00022758459260747887, 0.0005179474679231…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 425 ms, sys: 20.1 ms, total: 445 ms\n",
      "Wall time: 2.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.lr_explor(cost)>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# run the logisitc regression, optimization, regularization and cost based on user input\n",
    "from ipywidgets import widgets as wd\n",
    "from sklearn import metrics as mt\n",
    "    \n",
    "solver = input(\"Enter which optimization technique should be used: 1 - Steepest Descent, 2 - Stochastic, 3 - Quasi-Newton(BFGS) \")\n",
    "norm = input(\"Enter which regularization technique should be used: 1 - L1 Regularization, 2 - L2 Regularization, 3 - L1 and L2 Regularization, 4 - No Regularization \")\n",
    "\n",
    "def lr_explor(cost):\n",
    "    print('Running')\n",
    "    if(solver == '1'):\n",
    "        print(\"Steepest Descent Optimization selected\")\n",
    "        #default solver is Steepest Descent\n",
    "        lr = MultiClassLogisticRegression(eta=1,\n",
    "                                  iterations=5, norm=norm,\n",
    "                                  C=float(cost))\n",
    "    elif(solver == '2'):\n",
    "        print(\"SGD Optimization selected\")\n",
    "        lr = MultiClassLogisticRegression(eta=.01,\n",
    "                                  iterations=2000, norm=norm,\n",
    "                                  C=float(cost),\n",
    "                                  solver=StochasticLogisticRegression\n",
    "                                 )\n",
    "    else:\n",
    "        print(\"BFGS Quasi-Newton Optimization selected\")\n",
    "        lr = MultiClassLogisticRegression(_,\n",
    "                                  iterations=20, norm=norm,\n",
    "                                  C=float(cost),\n",
    "                                  solver=BFGSBinaryLogisticRegression\n",
    "                                 )\n",
    "    acc = []\n",
    "    for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "        lr.fit(X[train_indices],y[train_indices])  # train object\n",
    "        y_hat = lr.predict(X[test_indices]) # get test set predictions\n",
    "        acc.append(mt.accuracy_score(y[test_indices],y_hat))\n",
    "        \n",
    "    acc = np.array(acc)\n",
    "    print(acc.mean(),'+-',2.7*acc.std())\n",
    "        \n",
    "wd.interact(lr_explor,cost=list(np.logspace(-4,1,15)),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciKit-Learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.8580474934036939\n",
      "CPU times: user 26 ms, sys: 1.98 ms, total: 28 ms\n",
      "Wall time: 23.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# how do we compare now to sklearn?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_sk = LogisticRegression(solver='liblinear',n_jobs=1, \n",
    "                           multi_class='ovr', C = 1/0.001, \n",
    "                           penalty='l2',max_iter=100) \n",
    "\n",
    "lr_sk.fit(X,y) # no need to add bias term, sklearn does it internally!!\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
